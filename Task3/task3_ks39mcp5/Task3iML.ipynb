{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction To Machine Learning | Task 3 : Protein Classification task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import training, test set and a submission sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_sample = pd.read_csv('sample.csv')\n",
    "training_set = pd.read_csv('train.csv')\n",
    "test_set = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training set into labels and training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set contains Sequence and a flag marking the protein Active or not \n",
      "We have, in total, 112000 training data\n",
      "We don t have repeating sequences, since the length of unique sequences is 112000\n"
     ]
    }
   ],
   "source": [
    "print('Our training set contains', training_set.columns[0], 'and a flag marking the protein',training_set.columns[1],'or not ')\n",
    "print('We have, in total,', training_set.shape[0],'training data')\n",
    "x_train = training_set['Sequence']\n",
    "y_train = training_set['Active']\n",
    "print('We don t have repeating sequences, since the length of unique sequences is',pd.unique(x_train).shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The input to our classification task are sequence of letters, therefore we will have to figure out : \n",
    "* What is the way to represent them as meaningful numbers \n",
    "* Does order matter ? LSTM ? \n",
    "\n",
    "##### First of all lets undedrstand how many different aminoacids are there and their frequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:  DKWL ---> ['D', 'K', 'W', 'L']\n"
     ]
    }
   ],
   "source": [
    "#This function is used to split a sequence of letters into single letters\n",
    "def split(word):\n",
    "    return [char for char in word]\n",
    "\n",
    "print('Example: ',x_train[0],'--->',split(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'F', 'K', 'N', 'I', 'G', 'V', 'R', 'E', 'Q', 'L', 'M', 'A', 'Y', 'H', 'P', 'T', 'S', 'W', 'C']\n"
     ]
    }
   ],
   "source": [
    "single_letters = pd.DataFrame([split(code) for code in split(x_train)])\n",
    "#Collect the different letters found in any sequence, that is the different aminoacids\n",
    "aminoacids = [letters for letters in pd.unique(single_letters[0])]\n",
    "print(aminoacids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to count how often each aminoacid is there\n",
    "count = []\n",
    "for aminoacid in aminoacids: \n",
    "    count.append(sum(single_letters.to_numpy().flatten() == aminoacid))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aminoacids_frequency = np.array([aminoacids,count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize =(10,5))\n",
    "plt.bar(range(len(count)), count, align='center')\n",
    "plt.xticks(range(len(count)), aminoacids, size='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre Processing\n",
    "##### We create a dictionary containing all the different aminoacids that encode our mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D': 1, 'F': 2, 'K': 3, 'N': 4, 'I': 5, 'G': 6, 'V': 7, 'R': 8, 'E': 9, 'Q': 10, 'L': 11, 'M': 12, 'A': 13, 'Y': 14, 'H': 15, 'P': 16, 'T': 17, 'S': 18, 'W': 19, 'C': 20}\n",
      "Dict Length: 20\n"
     ]
    }
   ],
   "source": [
    "codes = aminoacids\n",
    "def create_dict(codes):\n",
    "    char_dict = {}\n",
    "    for index, val in enumerate(codes):\n",
    "        char_dict[val] = index+1\n",
    "    return char_dict\n",
    "\n",
    "char_dict = create_dict(codes)\n",
    "\n",
    "print(char_dict)\n",
    "print(\"Dict Length:\", len(char_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we encode our data with the ffollowing function, which for each protein, returns the sequence of aminoacidds according to our dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encoding(data):\n",
    "    \"\"\"\n",
    "  - Encodes code sequence to integer values.\n",
    "  - 20 common amino acids are taken into consideration\n",
    "    and rest 4 are categorized as 0.\n",
    "    \"\"\"\n",
    "    encode_list = []\n",
    "    for row in data['Sequence'].values:\n",
    "        row_encode = []\n",
    "        for code in row:\n",
    "            row_encode.append(char_dict.get(code, 0))\n",
    "        encode_list.append(np.array(row_encode))\n",
    "    \n",
    "    return encode_list\n",
    "  \n",
    "train_encoded = pd.DataFrame(integer_encoding(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thereafter we convert the codes to a onehot structure \n",
    "##### What this does is it simply transform each letter off each mutation sequence into one array of length 20 with a one in the column corresponding to the letter it encodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112000, 4, 21)\n",
      "112000\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# One hot encoding of sequences\n",
    "train_ohe = to_categorical(train_encoded)\n",
    "\n",
    "print(train_ohe.shape)\n",
    "print(y_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also have to transform our labels into a one hot encoding structure with the label being either zero or one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for row in y_train:\n",
    "    if row == 0 :\n",
    "        labels.append([1,0])\n",
    "    else :\n",
    "        labels.append([0,1])\n",
    "labels = np.array(labels)\n",
    "assert len(labels) == train_ohe.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We perform the same preprocessing pipeline for our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "Our test set contains Sequence\n",
      "We have, in total, 48000 test data\n",
      "\n",
      "We first split each sequence in a list of letters\n",
      "\n",
      "Example:  HWFK ---> ['H', 'W', 'F', 'K']\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "In the test set we have the followign Aminoacids  ['H', 'M', 'A', 'N', 'L', 'T', 'E', 'G', 'I', 'R', 'P', 'C', 'W', 'F', 'D', 'V', 'S', 'Y', 'K', 'Q']\n",
      "\n",
      "Are they equal to the trainin aminoacids ?        ['D', 'F', 'K', 'N', 'I', 'G', 'V', 'R', 'E', 'Q', 'L', 'M', 'A', 'Y', 'H', 'P', 'T', 'S', 'W', 'C']\n",
      "\n",
      "Yes they are. Therefore we can use the same dictionary to one hot encode\n",
      "\n",
      "{'H': 1, 'M': 2, 'A': 3, 'N': 4, 'L': 5, 'T': 6, 'E': 7, 'G': 8, 'I': 9, 'R': 10, 'P': 11, 'C': 12, 'W': 13, 'F': 14, 'D': 15, 'V': 16, 'S': 17, 'Y': 18, 'K': 19, 'Q': 20}\n",
      "\n",
      " Dict Length: 20\n",
      "\n",
      "---------------------------------------------------\n",
      "Example of encoded test set \n",
      "\n",
      " 0    HWFK\n",
      "1    MWPW\n",
      "2    ALDV\n",
      "3    NTLG\n",
      "4    LHYY\n",
      "Name: Sequence, dtype: object .\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "    0   1   2   3\n",
      "0  1  13  14  19\n",
      "1  2  13  11  13\n",
      "2  3   5  15  16\n",
      "3  4   6   5   8\n",
      "4  5   1  18  18\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Now we convert to one hot encoding, which has shape:\n",
      "\n",
      "(48000, 4, 21)\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------')\n",
    "print('Our test set contains', test_set.columns[0])\n",
    "print('We have, in total,', test_set.shape[0],'test data')\n",
    "\n",
    "x_test = test_set['Sequence']\n",
    "\n",
    "#This function is used to split a sequence of letters into single letters\n",
    "def split(word):\n",
    "    return [char for char in word]\n",
    "\n",
    "print('\\nWe first split each sequence in a list of letters\\n')\n",
    "\n",
    "print('Example: ',x_test[0],'--->',split(x_test[0]))\n",
    "\n",
    "single_letters = pd.DataFrame([split(code) for code in split(x_test)])\n",
    "\n",
    "#Collect the different letters found in any sequence, that is the different aminoacids\n",
    "test_aminoacids = [letters for letters in pd.unique(single_letters[0])]\n",
    "\n",
    "print('\\n---------------------------------------------------')\n",
    "print('\\nIn the test set we have the followign Aminoacids ',test_aminoacids)\n",
    "print('\\nAre they equal to the trainin aminoacids ?       ', aminoacids)\n",
    "print('\\nYes they are. Therefore we can use the same dictionary to one hot encode\\n')\n",
    "codes = test_aminoacids\n",
    "def create_dict(codes):\n",
    "    char_dict = {}\n",
    "    for index, val in enumerate(codes):\n",
    "        char_dict[val] = index+1\n",
    "    return char_dict\n",
    "\n",
    "char_dict = create_dict(codes)\n",
    "\n",
    "print(char_dict)\n",
    "print(\"\\n Dict Length:\", len(char_dict))\n",
    "\n",
    "def integer_encoding(data):\n",
    "    \"\"\"\n",
    "  - Encodes code sequence to integer values.\n",
    "  - 20 common amino acids are taken into consideration\n",
    "    and rest 4 are categorized as 0.\n",
    "    \"\"\"\n",
    "    encode_list = []\n",
    "    for row in data['Sequence'].values:\n",
    "        row_encode = []\n",
    "        for code in row:\n",
    "            row_encode.append(char_dict.get(code, 0))\n",
    "        encode_list.append(np.array(row_encode))\n",
    "    \n",
    "    return encode_list\n",
    "print('\\n---------------------------------------------------')\n",
    "\n",
    "test_encoded = pd.DataFrame(integer_encoding(test_set))\n",
    "\n",
    "print('Example of encoded test set \\n\\n', x_test[0:5],'.\\n.\\n.\\n.\\n.\\n.\\n\\n',test_encoded[0:5])\n",
    "print('\\n---------------------------------------------------')\n",
    "print('\\nNow we convert to one hot encoding, which has shape:\\n')\n",
    "\n",
    "test_ohe = to_categorical(test_encoded)\n",
    "\n",
    "print(test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Classifier Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Network Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation,Add,Input,Conv1D,MaxPooling1D,Dropout,Flatten,Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(data, filters, d_rate):\n",
    "    \"\"\"\n",
    "    _data: input\n",
    "    _filters: convolution filters\n",
    "    _d_rate: dilation rate\n",
    "    \"\"\"\n",
    "\n",
    "    shortcut = data\n",
    "\n",
    "    bn1 = BatchNormalization()(data)\n",
    "    act1 = Activation('relu')(bn1)\n",
    "    conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', kernel_regularizer=l2(0.001))(act1)\n",
    "\n",
    "    #bottleneck convolution\n",
    "    bn2 = BatchNormalization()(conv1)\n",
    "    act2 = Activation('relu')(bn2)\n",
    "    conv2 = Conv1D(filters, 3, padding='same', kernel_regularizer=l2(0.001))(act2)\n",
    "\n",
    "    #skip connection\n",
    "    x = Add()([conv2, shortcut])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "x_input = Input(shape=(4, 21))\n",
    "\n",
    "#initial conv\n",
    "conv = Conv1D(128, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "res1 = residual_block(conv, 128, 2)\n",
    "res2 = residual_block(res1, 128, 3)\n",
    "\n",
    "x = MaxPooling1D(3)(res2)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# softmax classifier\n",
    "x = Flatten()(x)\n",
    "x_output = Dense(2, activation='softmax', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "model2 = Model(inputs=x_input, outputs=x_output)\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4, 21)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 4, 128)       2816        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 4, 128)       512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 4, 128)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 4, 128)       16512       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4, 128)       512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 4, 128)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 4, 128)       49280       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4, 128)       0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 4, 128)       512         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 4, 128)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 4, 128)       16512       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 4, 128)       512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 4, 128)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 4, 128)       49280       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 4, 128)       0           conv1d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 128)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1, 128)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            258         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 136,706\n",
      "Trainable params: 135,682\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500/3500 [==============================] - 20s 5ms/step - loss: 0.3962 - accuracy: 0.9681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe77bb61e50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_ohe,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = model2.predict(train_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(probabilities,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9845357142857143\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = 1 - abs(sum(predictions - y_train)/len(y_train))\n",
    "print(training_accuracy)\n",
    "print(predictions[0],y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive =  2099 \n",
      "True Negative =  107405 \n",
      "False Positive =  382 \n",
      "False Negative =  2114\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "true_negative = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "for i in range(len(predictions)):\n",
    "    prediction = predictions[i]\n",
    "    truth = y_train[i]\n",
    "    if prediction == 1 and truth == 1 :\n",
    "        true_positive += 1\n",
    "    if prediction == 1 and truth == 0 :\n",
    "        false_positive += 1\n",
    "    if prediction == 0 and truth == 1 :\n",
    "        false_negative += 1\n",
    "    if prediction == 0 and truth == 0 :\n",
    "        true_negative += 1\n",
    "print('True Positive = ',true_positive,'\\nTrue Negative = ',true_negative,'\\nFalse Positive = ',false_positive,'\\nFalse Negative = ',false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is (on the training set) : 0.6271287720346579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('F1 score is (on the training set) :',f1_score(y_train, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.argmax(model2.predict(test_ohe),axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47994</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47995</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47997</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47998</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47999 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1\n",
       "0      0\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      0\n",
       "...   ..\n",
       "47994  0\n",
       "47995  1\n",
       "47996  0\n",
       "47997  1\n",
       "47998  0\n",
       "\n",
       "[47999 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
